# Runtime
APP_ENV=production
LOG_LEVEL=INFO
UVICORN_WORKERS=1

# API
REQUEST_TIMEOUT_SECONDS=120
MAX_UPLOAD_MB=20

# Model
HF_MODEL_ID=Qwen/Qwen-Image-Layered
HF_HOME=/root/.cache/huggingface
# Enable fallback only for debugging; keep false for true Qwen output
ENABLE_HEURISTIC_FALLBACK=false
# Enable low VRAM handling when GPU memory is under this threshold (GB)
LOW_VRAM_THRESHOLD_GB=20
# Default inference tuning used when frontend preset is not custom
DEFAULT_INFERENCE_STEPS=24
DEFAULT_RESOLUTION=512
DEFAULT_CFG_SCALE=3.0
# CPU dtype preference in Force CPU mode: bfloat16 (lower RAM) or float32 (max compatibility)
CPU_TORCH_DTYPE=bfloat16
# Optional Hugging Face token for gated/private models
HF_TOKEN=

# Redis / Celery
REDIS_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1

# Output storage inside container
OUTPUT_DIR=/tmp/output
OUTPUT_RETENTION_SECONDS=3600
TASK_STALE_SECONDS=900
TASK_SOFT_TIME_LIMIT_SECONDS=1800
TASK_HARD_TIME_LIMIT_SECONDS=2100
BROKER_VISIBILITY_TIMEOUT_SECONDS=3600
# Worker status heartbeat interval while inference is running
INFERENCE_HEARTBEAT_SECONDS=20

# Docker build/runtime
PRELOAD_MODEL=false
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
