# Runtime
APP_ENV=production
LOG_LEVEL=INFO

# API
REQUEST_TIMEOUT_SECONDS=120
MAX_UPLOAD_MB=20

# Model
HF_MODEL_ID=Qwen/Qwen-Image-Layered
HF_HOME=/root/.cache/huggingface
# Enable fallback only for debugging; keep false for true Qwen output
ENABLE_HEURISTIC_FALLBACK=false
# Enable low VRAM handling when GPU memory is under this threshold (GB)
LOW_VRAM_THRESHOLD_GB=20
# Default inference tuning used when frontend preset is not custom
DEFAULT_INFERENCE_STEPS=24
DEFAULT_RESOLUTION=512
DEFAULT_CFG_SCALE=3.0
# Optional Hugging Face token for gated/private models
HF_TOKEN=

# Redis / Celery
REDIS_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1

# Output storage inside container
OUTPUT_DIR=/tmp/output
OUTPUT_RETENTION_SECONDS=3600

# Docker build/runtime
PRELOAD_MODEL=false
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
